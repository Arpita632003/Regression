{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7247793c-b2f6-4150-bd9e-00eff0cc9c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "What is Simple Linear Regression\n",
    "Simple Linear Regression is a statistical method used to understand the relationship between two continuous variables: one independent variable (predictor) and one dependent variable (response).\n",
    "The goal is to find the best-fitting straight line that predicts the dependent variable based on the independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58238c0-a07d-45be-a81b-5f8134378f73",
   "metadata": {},
   "outputs": [],
   "source": [
    " What are the key assumptions of Simple Linear Regression\n",
    "Linear Relationship\n",
    "\n",
    "Independence\n",
    "Homoscedasticity\n",
    "Normality\n",
    "No Multicollinearity\n",
    "\n",
    "No Endogeneity\n",
    "\n",
    "Number of Observations\n",
    "\n",
    "Unique Observations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dbf7cc-4264-4381-92ed-d1c8c80b7dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    " What does the coefficient m represent in the equation Y=mX+c\n",
    "Linear equations are fundamental to many areas of mathematics, engineering, physics, and business. A linear equation in two variables is an equation that can be written in the form y=mx+c, where ‘y’ and ‘x’ are the variables, ‘m’ is the slope, and ‘c’ is the y-intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0501a32e-4d5f-4a8d-901d-07cae9d9142e",
   "metadata": {},
   "outputs": [],
   "source": [
    " What does the intercept c represent in the equation Y=mX+c\n",
    "The value of c in the equation y = mx + c represents the y-intercept of the line. The intercept is the distance from the origin on the y-axis, where this line cuts the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ec6944-21e3-4f0d-a7b7-8110f80730ab",
   "metadata": {},
   "outputs": [],
   "source": [
    " How do we calculate the slope m in Simple Linear Regression\n",
    "The formula for calculating the slope (m) in simple linear regression is m = (n∑x y - (∑x) (∑y)) / (n*******∑x 2 - (∑x) 2)**. The equation of a simple linear regression line (the line of best fit) is y = mx + b. In this equation, Y is the response (dependent) variable, X is the predictor (independent) variable, m is the estimated slope, and b is the estimated intercept. The slope (m) is calculated using the formula above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a0ae9d-84e6-4336-bfd1-31291814e181",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is the purpose of the least squares method in Simple Linear Regression\n",
    "The least square method is a statistical technique used to find the line of best fit for a set of data points by minimizing the sum of the squares of the residuals (the differences between the observed and predicted values).\n",
    "This method is widely used in regression analysis to predict the behavior of the dependent variable based on the independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3ddc84-d1b1-4523-ba51-74ea8b296a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    " How is the coefficient of determination (R²) interpreted in Simple Linear Regression\n",
    "# Create data frame\n",
    "df <- data.frame(hours=c(1, 2, 2, 4, 2, 1, 5, 4, 2, 4, 4, 3, 6, 5, 3),\n",
    "prep_exams=c(1, 3, 3, 5, 2, 2, 1, 1, 0, 3, 4, 3, 2, 4, 4),\n",
    "score=c(76, 78, 85, 88, 72, 69, 94, 94, 88, 92, 90, 75, 96, 90, 82))\n",
    "\n",
    "# Fit regression model\n",
    "model <- lm(score ~ hours + prep_exams, data = df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb512bd5-3408-4c20-9a19-5f8e762bbb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is Multiple Linear Regression\n",
    "Multiple linear regression is a statistical method used to model the relationship between multiple independent variables and a single dependent variable. It extends simple linear regression by using more than one predictor variable to predict the outcome.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543c6f0f-f66c-4a16-b875-6db6ab479939",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is the main difference between Simple and Multiple Linear Regression\n",
    "Linear (Simple) Regression\n",
    "Models the relationship between one dependent and one independent variable\n",
    "Y = C0 + C1X + e\n",
    "It is simpler to deal with one relationship\n",
    "Linearity, Independence, Homoscedasticity, Normality\n",
    "Multiple Regression\n",
    "Models the relationship between one dependent and two or more independent variables\n",
    "Y = C0 + C1X1 + C2X2 + C3X3 + ….. + CnXn + e\n",
    "More complex due to multiple relationships.\n",
    "Same as linear regression, with the added concern of multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e60457-dcd4-439f-9515-e90186dc9c54",
   "metadata": {},
   "outputs": [],
   "source": [
    " What are the key assumptions of Multiple Linear Regression\n",
    "Linear relationship: There must be a linear relationship between the outcome variable and the independent variables.\n",
    "Multivariate normality: The residuals are normally distributed.\n",
    "No or little multicollinearity: The independent variables should not be too highly correlated with each other.\n",
    "No auto-correlation: There is no correlation between consecutive residuals in time series data.\n",
    "Homoscedasticity: The variance of the residuals is constant across all levels of the independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d571f96e-2a49-4f40-9574-6f8613e9c78b",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model\n",
    "Heteroscedasticity, sometimes spelled heteroskedasticity, refers to the unequal scatter of residuals or error terms in regression analysis. Specifically, it indicates a systematic change in the spread of the residuals over the range of measured values\n",
    "1\n",
    ". This phenomenon violates one of the key assumptions of ordinary least squares (OLS) regression, which assumes that the residuals have constant variance, known as homoscedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d657760-1068-4990-aa24-6d5dc3d0e844",
   "metadata": {},
   "outputs": [],
   "source": [
    " How can you improve a Multiple Linear Regression model with high multicollinearity\n",
    " Remove one or more of the highly correlated variables. ...\n",
    "2. Linearly combine the predictor variables in some way, such as adding or subtracting them from one way. ...\n",
    "3. Perform an analysis that is designed to account for highly correlated variables such as principal component analysis or partial least squares (PLS) regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4559e7-f33c-4f75-a433-543ea591734d",
   "metadata": {},
   "outputs": [],
   "source": [
    " What are some common techniques for transforming categorical variables for use in regression models\n",
    "One-Hot Encoding: Convert categorical variables into binary columns, where each column corresponds to a unique category of the variable.\n",
    "Regression: Once the categorical variables are encoded, they can be used as features (independent variables) in a regression model.\n",
    "Fit a Linear Regression Model: Use the encoded features along with a target variable to fit a linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daff721d-ffdc-4e59-afe8-ae2a5a210596",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is the role of interaction terms in Multiple Linear Regression\n",
    "Considering interactions in multiple linear regression is crucial for gaining a fuller understanding of the relationships between predictors and preventing misleading interpretations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451cd80d-0fee-4fce-aeb2-1c470132d536",
   "metadata": {},
   "outputs": [],
   "source": [
    " How can the interpretation of intercept differ between Simple and Multiple Linear Regression\n",
    "Simple linear regression\n",
    "ŷ = β0 + β1(x)\n",
    "\n",
    "where:\n",
    "\n",
    "ŷ: The predicted value for the response variable\n",
    "β0: The mean value of the response variable when x = 0\n",
    "β1: The average change in the response variable for a one unit increase in x\n",
    "x: The value for the predictor variable\n",
    "A multiple linear regression model takes the following form:\n",
    "\n",
    "ŷ = β0 + β1(x1) + β2(x2) + β3(x3) + … + βk(xk)\n",
    "\n",
    "where:\n",
    "\n",
    "ŷ: The predicted value for the response variable\n",
    "β0: The mean value of the response variable when all predictor variables are zero\n",
    "βj: The average change in the response variable for a one unit increase in the jth predictor variable, assuming all other predictor variables are held constant\n",
    "xj: The value for the jth predictor variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ddfff8-d1d3-4982-b8c2-5ec34fa6b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is the significance of the slope in regression analysis, and how does it affect predictions\n",
    "The slope of the regression line indicates how much the dependent variable is expected to change for a one-unit change in the independent variable. This insight is essential for making predictions and understanding the strength and direction of relationships within data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b652f210-fa14-4133-84bf-dddea8d6efce",
   "metadata": {},
   "outputs": [],
   "source": [
    " How does the intercept in a regression model provide context for the relationship between variables\n",
    "The coefficients in the equation define the relationship between each independent variable and the dependent variable. The intercept or constant in the regression model represents the mean value of the response variable when all the predictor variables in the model are equal to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83eb63d-1c54-4edf-9d6c-1262f59e5323",
   "metadata": {},
   "outputs": [],
   "source": [
    " What are the limitations of using R² as a sole measure of model performance\n",
    "While R² provides valuable insights into model performance, it has limitations. It can be misleading for models that overfit the data, as a high R² might indicate that the model is too complex and captures noise rather than true patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd949c5-471a-484f-881a-9092204fb88b",
   "metadata": {},
   "outputs": [],
   "source": [
    " How would you interpret a large standard error for a regression coefficient\n",
    "The standard error of the regression (S) is a crucial metric in linear regression analysis. It measures the average distance that the observed values fall from the regression line. This metric is particularly useful for assessing the precision of predictions made by the regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2324f12e-532d-4ebc-ab12-ca5661042d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    " How can heteroscedasticity be identified in residual plots, and why is it important to address it\n",
    "Pure heteroscedasticity refers to cases where you specify the correct model and yet you observe non-constant variance in the residual plots. Impure heteroscedasticity refers to cases where you incorrectly specify the model, and that causes the non-constant variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b99483-1c77-4cf1-9e64-a79c7cf9555a",
   "metadata": {},
   "outputs": [],
   "source": [
    " What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²\n",
    "R-squared measures the proportion of variance in the dependent variable explained by the model's independent variable or variables. The adjusted r-squared also explains the variance, but it has an additional consideration: As the name suggests, it adjusts the r-squared value by penalizing the inclusion of irrelevant, by which we mean highly collinear, variables. It does this by taking into account both the number of predictors and the sample size.\n",
    "\n",
    "In order to find the adjusted r-squared, we have to first find the r-squared. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8b9ec7-77ff-4c44-a1c5-a2ce6256dba0",
   "metadata": {},
   "outputs": [],
   "source": [
    " Why is it important to scale variables in Multiple Linear Regression\n",
    "n regression, normalization can impact the performance and stability of your model. If the dataset has features with vastly different scales (for example, income in thousands and age in years), normalizing those features can make the model more efficient and reliable. Normalization doesn’t change the data’s overall distribution but rather adjusts the values so that each feature contributes equally, preventing any single feature from dominating due to its scale. Normalization is crucial in regression techniques that use gradient descent to optimize, as it helps these models converge faster and avoid getting stuck in local minima.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338a4a50-61b6-4535-96b7-4269a3eb9be2",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is polynomial regression\n",
    "Polynomial regression is a type of regression analysis used in statistics and machine learning to model the relationship between a dependent variable ( y ) and an independent variable ( x ) as an ( n )-degree polynomial. Unlike simple linear regression, which models the relationship as a straight line, polynomial regression can capture non-linear patterns in the data by fitting a polynomial equation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3e639c-03cd-4ffd-8b75-ca08bf29d703",
   "metadata": {},
   "outputs": [],
   "source": [
    " How does polynomial regression differ from linear regression\n",
    "Both linear and polynomial regression have their places in predictive modeling. Linear regression is simpler and works well for linear relationships, while polynomial regression is more flexible and can model more complex relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8197f05c-1948-449b-9cdf-deef0c2ededd",
   "metadata": {},
   "outputs": [],
   "source": [
    " When is polynomial regression used\n",
    "Polynomial linear regression is a type of regression analysis used to model the relationship between a dependent variable (y) and an independent variable (x) as an nth-degree polynomial. Unlike simple linear regression, which models the relationship as a straight line, polynomial regression can capture non-linear patterns in the data by fitting a polynomial equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459c6586-cbcc-4d3f-912b-1f5b6d3fd294",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is the general equation for polynomial regression\n",
    "The general equation of a polynomial regression model is:\n",
    "\n",
    "y = a0 + a1x + a2x2 + … + anxn + ϵ\n",
    "\n",
    "Here,\n",
    "\n",
    "a0, a1, …, an are the coefficients of the polynomial terms\n",
    "n is the degree of the polynomial\n",
    "ϵ is the error term, representing deviations between predicted and actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2336f62-6e92-488c-add9-ebd061f5cbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    " Can polynomial regression be applied to multiple variables\n",
    "Polynomial regression can be used for multiple independent variables, which is called multivariate polynomial regression. These equations are usually very complex but give us more flexibility and higher accuracy due to utilizing multiple variables in the same equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176a695b-fffd-48ec-a073-11e28db9ae26",
   "metadata": {},
   "outputs": [],
   "source": [
    " What are the limitations of polynomial regression\n",
    "Overfitting: If we include too many polynomial features (i.e., increase the degree of the polynomial), our model can become too complex. ...\n",
    "Underfitting: On the other hand, if we don’t include enough polynomial features (i.e., the degree of the polynomial is too low), our model may be too simple to capture the underlying trend. ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a9919c-0526-4be3-8be3-e660bb6c4acc",
   "metadata": {},
   "outputs": [],
   "source": [
    " What methods can be used to evaluate model fit when selecting the degree of a polynomial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdc272b-d1f9-4290-9725-0c508dcafd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    " Why is visualization important in polynomial regression\n",
    "What values are central or typical? (e.g., mean, median, modes)\n",
    "What is the typical spread of values around those central values? (e.g., variance/stdev, skewness)\n",
    "What are unusual or exceptional values (e.g., outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aded589-08d6-4128-b83a-923c068c1998",
   "metadata": {},
   "outputs": [],
   "source": [
    " How is polynomial regression implemented in Python?\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# Sample data\n",
    "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)\n",
    "y = np.array([1, 4, 9, 16, 25, 36, 49, 64, 81, 100])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
